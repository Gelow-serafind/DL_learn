# Deep Learning

本页用于记录深度学习的学习过程，所用的教材为《Dive Into Deep Learning》。

# 1.前言

略

# 2.预备知识

在本教程的学习上，我们采用的框架为PyTorch框架，语言为Python无可厚非。基础的Python语法不再赘述，基础的PyTorch用法其实也不太重要，以下仅可能列举几个先前没听说过的用法。

因此预备知识包括PyTorch的一些常见用法，以及线性代数的基础理论

### PyTorch基本用法

### 张量广播机制

倘若遇到以下情况，需要将张量A与B相加：

```python
A = torch.arange(3).reshape((3, 1))
B = torch.arange(2).reshape((1, 2))
```

此时张量A与B的值分别为：

```python
tensor([[0],
        [1],
        [2]])
tensor([[0, 1]])
```

很明显，张量A与B并不是一个形状的。那么PyTorch会运用自身的广播机制将两个张量相加，进而得到如下结果。

```python
tensor([[0, 1],
        [1, 2],
        [2, 3]])
```

简单来说就是矩阵a将复制列，矩阵b将复制行，然后再按元素相加。

**规范点的描述就是：**

假设我们有两个张量，它们的形状分别为 ：

`(a_1, a_2, ..., a_n)` 和 `(b_1, b_2, ..., b_m)`

如果需要对它们进行算术运算，PyTorch 会遵循以下广播规则：

1. **匹配维度：** 从右到左比较两个张量的每个维度。如果维度相同，或者其中一个维度为1，则这两个维度是兼容的。
2. **自动扩展：** 如果一个张量的某个维度是1，那么这个张量在该维度上会被“扩展”到与另一个张量相同的维度。扩展后的张量将会重复数据，以匹配另一个张量的形状。
3. **插入新维度：** 如果两个张量的维度数不同，维度数较少的张量将在左侧插入足够多的“1”以匹配维度数较多的张量。

简单来说就是，先看看哪个维度一样，就以那个维度为准，对其他维度进行扩展，扩展的形式就是复制一遍本身的内容。如果实在是找不到一样的维度，那就会引发错误。*因此，广播机制也不是能将所有的不同形状的张量放一起运算的。*

### 数据集读取

为了能用深度学习来解决现实世界的问题，我们经常从预处理原始数据开始，而不是从那些准备好的张量格式数据开始。在Python中常用的数据分析工具中，我们通常使用pandas软件包。像庞大的Python生态系统中的许多其他扩展包一样，pandas可以与张量兼容。

### 线性代数

### 矩阵

在线性代数的领域中，最标志性的数学表达方式就是矩阵，我们可以使用PyTorch的语法实例化矩阵

```python
A = torch.arange(20).reshape(5, 4)
```

那么此时A表达的就是一个5行4列的矩阵，或者行列式，都可以，这取决于你怎么认为他以及后续拿来作何种运算

```python
A = tensor([[ 0, 1, 2, 3],
		        [ 4, 5, 6, 7],
            [ 8, 9, 10, 11],
            [12, 13, 14, 15],
            [16, 17, 18, 19]]
```

$$
A=\begin{bmatrix} 0&1&2&3 \\ 4&5&6&7 \\ 8&9&10&11\\12&13&14&15\\16&17&18&19\end{bmatrix}

$$

矩阵在计算机编码中实际上就是数组，因此可以通过下标对其进行访问。

### 矩阵的转置

假设有个矩阵A

$$
A=\begin{bmatrix} 0&1&2&3 \\ 4&5&6&7 \\ 8&9&10&11\\12&13&14&15\\16&17&18&19\end{bmatrix}

$$

那么A的转置就是

$$
\begin{bmatrix} 
0 & 4 & 8 & 12 & 16 \\ 
1 & 5 & 9 & 13 & 17 \\ 
2 & 6 & 10 & 14 & 18 \\ 
3 & 7 & 11 & 15 & 19 
\end{bmatrix}

$$

很明显可以看出来，矩阵的转置就是行变列，列变行，以斜线为轴，进行对称旋转。**对称矩阵则是转置前后都一致的矩阵。**

在PyTorch里面使用

```python
A.T
```

来访问矩阵A的转置

```python
A.T= tensor([[ 0,  4,  8, 12, 16],
             [ 1,  5,  9, 13, 17],
             [ 2,  6, 10, 14, 18],
             [ 3,  7, 11, 15, 19]])
```

### 张量

在矩阵的上一级，有向量，表示的是一组带方向的数据，与脑海中概念里面的向量意义是一致的。

为了表示更丰富的数据结构，我们后面用张量统一表示这些数据，并且更高维度的数据结构，也统一用张量表示。

例如

```python
A = torch.arange(36).reshape(3,3,4)
```

生成一个张量A

```python
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]],

        [[24, 25, 26, 27],
         [28, 29, 30, 31],
         [32, 33, 34, 35]]])
```

这个张量就类似于图像处理里面的**RGB**三个通道的数据

### 张量的点积

张量的点积（也称为内积）是一种将两个张量的对应元素相乘并累加的操作。这通常用于计算向量之间的相似性或者在神经网络中计算加权和。

以下是一个简单的例子，展示如何计算两个向量的点积：

```python
import torch
A = torch.tensor([1, 2, 3])
B = torch.tensor([4, 5, 6])
Dot_AB = torch.dot(A, B)
print(Dot_AB)  # 输出 32
```

在这个例子中，`A` 和 `B` 是两个一维张量，它们的点积计算如下：

PyTorch 提供了 `torch.dot` 函数来方便地计算两个一维张量的点积。如果是二维或更高维度的张量，可以使用 `torch.matmul` 或 `torch.bmm` 等函数来进行矩阵乘法。

### 矩阵-向量积

矩阵-向量积是指将一个矩阵与一个向量相乘，得到一个新的向量。具体来说，如果我们有一个矩阵 A 和一个向量 x，它们的矩阵-向量积 Ax 将生成一个新的向量。计算方法是将矩阵的每一行与向量相乘并求和。

以下面为例。假设有个矩阵

$$
A=\begin{bmatrix} 1&2&3 \\ 4&5&6\end{bmatrix}
$$

又有一个向量

$$
x=\begin{bmatrix}1\\2\\3\end{bmatrix}
$$

那么他们的矩阵-向量积就是：

$$
A·x = \begin{bmatrix}1+4+9\\4+10+18\end{bmatrix}=\begin{bmatrix}14\\32\end{bmatrix}
$$

计算过程就是将每一行的元素分别与向量的对应行元素相乘并相加，最终得出一个新的向量。**抽象地可以想象为将向量横过来相乘相加。**

```python
import torch

# 定义矩阵 A 和向量 x
A = torch.tensor([[1, 2, 3], [4, 5, 6]])
x = torch.tensor([1, 2, 3])

# 计算矩阵-向量积
Ax = torch.matmul(A, x)
print(Ax)  # 输出 tensor([14, 32])
```

矩阵-向量积在深度学习中是实现线性变换、特征提取和梯度计算等操作的基础，广泛应用于神经网络的构建和训练。例如，在一个简单的神经网络中，输入图像的像素值可以通过矩阵-向量积与权重矩阵相乘，得到隐藏层的激活值，从而实现图像分类。

### 矩阵-矩阵乘法

矩阵-矩阵乘法是指将两个矩阵相乘，得到一个新的矩阵。具体来说，如果我们有两个矩阵 A 和 B，它们的矩阵乘法 AB 将生成一个新的矩阵。计算方法是将矩阵 A 的每一行与矩阵 B 的每一列相乘并求和。

以下面为例。假设有两个矩阵

$$
A=\begin{bmatrix} 1&2&3 \\ 4&5&6 \end{bmatrix}
$$

和

$$
B=\begin{bmatrix} 7&8 \\ 9&10 \\ 11&12 \end{bmatrix}

$$

那么它们的矩阵乘法就是：

$$
AB = \begin{bmatrix} 1×7+2×9+3×11 & 1×8+2×10+3×12 \\ 4×7+5×9+6×11 & 4×8+5×10+6×12 \end{bmatrix} = \begin{bmatrix} 58 & 64 \\ 139 & 154 \end{bmatrix}

$$

计算过程是将矩阵 A 的每一行与矩阵 B 的每一列对应元素相乘并相加，最终得到一个新的矩阵。

在 PyTorch 中，可以使用 `torch.matmul` 函数来进行矩阵-矩阵乘法：

```python
import torch

# 定义矩阵 A 和矩阵 B
A = torch.tensor([[1, 2, 3], [4, 5, 6]])
B = torch.tensor([[7, 8], [9, 10], [11, 12]])

# 计算矩阵-矩阵乘法
AB = torch.matmul(A, B)
print(AB)  # 输出 tensor([[ 58,  64], [139, 154]])
```

矩阵-矩阵乘法在深度学习中是实现卷积操作、特征提取和梯度计算等操作的基础，并广泛应用于神经网络的构建和训练。例如，在卷积神经网络中，滤波器和输入特征图的卷积操作可以通过矩阵乘法来实现，从而提取输入数据的高级特征。

### 自动微分

在深度学习中，自动微分（Automatic Differentiation）是一种用于计算函数导数的技术。它在训练神经网络时尤其重要，因为我们需要通过反向传播算法来更新模型的参数，而反向传播依赖于计算损失函数相对于每个参数的导数。

PyTorch 提供了内置的自动微分功能，使得我们能够轻松地计算张量的梯度。在 PyTorch 中，`torch.autograd` 模块是实现自动微分的核心。

以下是一个简单的例子，展示如何使用 PyTorch 进行自动微分：

```python
import torch

# 创建一个张量并设置 requires_grad=True 以便跟踪其梯度
x = torch.tensor([2.0, 3.0], requires_grad=True)

# 定义一个简单的函数
y = x[0]**2 + x[1]**3

# 计算梯度
y.backward()

# 输出梯度
print(x.grad)  # 输出 tensor([4., 27.])
```

在这个例子中，我们首先创建了一个张量 `x`，并设置 `requires_grad=True` 以便 PyTorch 跟踪其梯度。然后我们定义了一个简单的函数 `y`，它是 `x` 的一个函数。通过调用 `y.backward()`，我们计算了 `y` 相对于 `x` 的梯度，并通过 `x.grad` 获取计算出的梯度值。

自动微分在深度学习中至关重要，因为它使得我们能够高效地计算梯度，从而加速模型训练过程。PyTorch 的自动微分功能使得我们能够轻松地实现复杂的模型，并进行有效的参数优化。

### 概率